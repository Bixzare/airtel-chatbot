# Syst√®me RAG - Airtel Chatbot

## üîç Vue d'ensemble

Le syst√®me RAG (Retrieval-Augmented Generation) est le c≈ìur du chatbot Airtel Niger. Il combine la recherche vectorielle avec la g√©n√©ration de texte pour fournir des r√©ponses pr√©cises bas√©es sur la connaissance d'Airtel.

## üèóÔ∏è Architecture RAG

### 1. Pipeline RAG complet

```
Document ‚Üí Chunking ‚Üí Embeddings ‚Üí Vector Store ‚Üí Query ‚Üí Retrieval ‚Üí Generation ‚Üí Response
```

### 2. Composants principaux

#### Document Processor
**Fichier** : `backend/src/rag/document_processor.py`

```python
class DocumentProcessor:
    def __init__(self, chunk_size: int = 400, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
```

**Fonctionnalit√©s :**
- **Chunking intelligent** : D√©coupage des documents en chunks optimis√©s
- **Pr√©servation du contexte** : Overlap entre chunks pour maintenir la coh√©rence
- **M√©tadonn√©es** : Conservation des informations de source
- **Multi-format** : Support TXT, PDF, DOCX

#### Vector Store
**Fichier** : `backend/src/rag/vector_store.py`

```python
class VectorStore:
    def __init__(self, embedding_dim: int = 768):
        self.embedding_dim = embedding_dim
        self.documents = []
        self.embeddings = []
        self.index = None
        self.embedding_model = GoogleEmbeddings()
```

**Fonctionnalit√©s :**
- **Embeddings Google** : Utilisation de text-embedding-004
- **Index FAISS** : Recherche vectorielle rapide
- **Similarit√© cosinus** : Calcul de similarit√© optimis√©
- **Persistance** : Sauvegarde et chargement des index

#### RAG Tool
**Fichier** : `backend/src/tools/rag_tool.py`

```python
class RAGTool(BaseTool):
    def __init__(self, document_path: str, additional_documents: List[str] = None):
        self.processor = DocumentProcessor(chunk_size=400, chunk_overlap=50)
        self.vector_store = VectorStore(embedding_dim=768)
        self.cache = RAGCache() if settings.rag_cache_enabled else None
```

## ‚öôÔ∏è Configuration

### Variables d'environnement

```bash
# Document principal
DOCUMENT_PATH=src/rag/static_document.txt

# Documents additionnels (optionnel)
ADDITIONAL_DOCUMENTS=doc1.txt,doc2.txt

# Param√®tres RAG
CHUNK_SIZE=400
CHUNK_OVERLAP=50
MAX_RESULTS=2
SIMILARITY_THRESHOLD=0.7

# Cache RAG
RAG_CACHE_ENABLED=true
RAG_CACHE_SIZE=1000
RAG_CACHE_TTL=3600
```

### Param√®tres optimis√©s

```python
# Dans src/config/settings.py
class Settings:
    # RAG Configuration
    chunk_size: int = 400
    chunk_overlap: int = 50
    max_results: int = 2
    similarity_threshold: float = 0.7
    
    # Cache Configuration
    rag_cache_enabled: bool = True
    rag_cache_size: int = 1000
    rag_cache_ttl: int = 3600
```

## üîÑ Workflow RAG

### 1. Pr√©paration des documents

```python
def prepare_documents(document_path: str) -> List[Document]:
    """Pr√©parer les documents pour l'indexation."""
    # Charger le document
    with open(document_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # D√©couper en chunks
    chunks = text_splitter.split_text(content)
    
    # Cr√©er les objets Document
    documents = []
    for i, chunk in enumerate(chunks):
        doc = Document(
            page_content=chunk,
            metadata={
                "source": document_path,
                "chunk_id": i,
                "chunk_size": len(chunk)
            }
        )
        documents.append(doc)
    
    return documents
```

### 2. G√©n√©ration des embeddings

```python
def generate_embeddings(documents: List[Document]) -> List[List[float]]:
    """G√©n√©rer les embeddings pour tous les chunks."""
    embeddings = []
    
    for doc in documents:
        # G√©n√©rer l'embedding
        embedding = embedding_model.embed_query(doc.page_content)
        embeddings.append(embedding)
    
    return embeddings
```

### 3. Recherche vectorielle

```python
def similarity_search(query: str, k: int = 2) -> List[Document]:
    """Rechercher les documents les plus similaires."""
    # G√©n√©rer l'embedding de la requ√™te
    query_embedding = embedding_model.embed_query(query)
    
    # Recherche dans l'index FAISS
    distances, indices = index.search(
        np.array([query_embedding]), k
    )
    
    # R√©cup√©rer les documents correspondants
    results = []
    for idx in indices[0]:
        if idx < len(documents):
            results.append(documents[idx])
    
    return results
```

### 4. G√©n√©ration de r√©ponse

```python
def generate_response(query: str, context_docs: List[Document]) -> str:
    """G√©n√©rer une r√©ponse bas√©e sur le contexte."""
    # Pr√©parer le contexte
    context = "\n\n".join([doc.page_content for doc in context_docs])
    
    # Cr√©er le prompt
    prompt = f"""
    Contexte sur Airtel Niger:
    {context}
    
    Question: {query}
    
    R√©ponse bas√©e uniquement sur le contexte fourni:
    """
    
    # G√©n√©rer la r√©ponse
    response = llm.invoke(prompt)
    return response.content
```

## üöÄ Optimisations RAG

### 1. Cache intelligent

**Fichier** : `backend/src/rag/cache.py`

```python
class RAGCache:
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.cache = {}
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.access_times = {}
        self.hit_count = 0
        self.miss_count = 0
    
    def get(self, query: str) -> Optional[List[str]]:
        """R√©cup√©rer un r√©sultat du cache."""
        if query in self.cache:
            # V√©rifier le TTL
            if time.time() - self.access_times[query] < self.ttl_seconds:
                self.hit_count += 1
                self.access_times[query] = time.time()
                return self.cache[query]
            else:
                # Expir√©, supprimer
                del self.cache[query]
                del self.access_times[query]
        
        self.miss_count += 1
        return None
    
    def set(self, query: str, results: List[str]):
        """Stocker un r√©sultat dans le cache."""
        # Gestion de la taille max
        if len(self.cache) >= self.max_size:
            self._evict_oldest()
        
        self.cache[query] = results
        self.access_times[query] = time.time()
    
    def get_stats(self) -> dict:
        """Obtenir les statistiques du cache."""
        total_requests = self.hit_count + self.miss_count
        hit_rate = self.hit_count / total_requests if total_requests > 0 else 0
        
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "hit_count": self.hit_count,
            "miss_count": self.miss_count,
            "hit_rate": hit_rate,
            "ttl_seconds": self.ttl_seconds
        }
```

### 2. Recherche hybride

```python
def hybrid_search(query: str, k: int = 2) -> List[Document]:
    """Recherche combinant similarit√© vectorielle et mots-cl√©s."""
    # Recherche vectorielle
    vector_results = similarity_search(query, k)
    
    # Recherche par mots-cl√©s
    keyword_results = keyword_search(query, k)
    
    # Combiner et d√©dupliquer
    combined_results = combine_results(vector_results, keyword_results)
    
    # Trier par score combin√©
    scored_results = score_results(combined_results, query)
    
    return scored_results[:k]
```

### 3. Pr√©chargement optimis√©

```python
def preload_documents():
    """Pr√©charger les documents pour optimiser les performances."""
    logger.info("Starting document preloading...")
    
    # Charger le document principal
    document_path = os.environ.get("DOCUMENT_PATH", "src/rag/static_document.txt")
    
    if os.path.exists(document_path):
        documents = processor.load_file(document_path)
        vector_store.add_documents(documents)
        logger.info(f"Loaded {len(documents)} chunks from {document_path}")
    else:
        logger.warning(f"Document not found: {document_path}")
    
    # Charger les documents additionnels
    additional_docs = os.environ.get("ADDITIONAL_DOCUMENTS", "").split(",")
    for doc_path in additional_docs:
        if doc_path.strip() and os.path.exists(doc_path.strip()):
            try:
                docs = processor.load_file(doc_path.strip())
                vector_store.add_documents(docs)
                logger.info(f"Loaded {len(docs)} chunks from {doc_path}")
            except Exception as e:
                logger.error(f"Error loading {doc_path}: {e}")
    
    # Test de r√©chauffement
    test_query = "Airtel services"
    test_results = vector_store.similarity_search(test_query, k=1)
    logger.info(f"Preloading test completed: {len(test_results)} results")
```

## üìä Monitoring RAG

### 1. M√©triques de performance

```python
class RAGMetrics:
    def __init__(self):
        self.total_queries = 0
        self.cache_hits = 0
        self.cache_misses = 0
        self.average_response_time = 0
        self.embedding_generation_time = 0
        self.search_time = 0
    
    def update_metrics(self, query_time: float, cache_hit: bool):
        """Mettre √† jour les m√©triques."""
        self.total_queries += 1
        
        if cache_hit:
            self.cache_hits += 1
        else:
            self.cache_misses += 1
        
        # Mettre √† jour le temps moyen
        self.average_response_time = (
            (self.average_response_time * (self.total_queries - 1) + query_time) 
            / self.total_queries
        )
    
    def get_metrics(self) -> dict:
        """Obtenir toutes les m√©triques."""
        cache_hit_rate = (
            self.cache_hits / self.total_queries 
            if self.total_queries > 0 else 0
        )
        
        return {
            "total_queries": self.total_queries,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "cache_hit_rate": cache_hit_rate,
            "average_response_time": self.average_response_time,
            "embedding_generation_time": self.embedding_generation_time,
            "search_time": self.search_time
        }
```

### 2. Endpoint de monitoring

```python
@app.get("/rag-metrics")
async def get_rag_metrics():
    """Obtenir les m√©triques RAG."""
    return {
        "rag_metrics": rag_metrics.get_metrics(),
        "cache_stats": rag_tool.cache.get_stats() if rag_tool.cache else None,
        "vector_store": {
            "total_documents": len(vector_store.documents),
            "embedding_dimension": vector_store.embedding_dim,
            "index_size": vector_store.index.ntotal if vector_store.index else 0
        }
    }
```

## üîß Personnalisation

### 1. Ajout de nouveaux documents

```python
def add_document(document_path: str):
    """Ajouter un nouveau document au syst√®me RAG."""
    if not os.path.exists(document_path):
        raise FileNotFoundError(f"Document not found: {document_path}")
    
    # Traiter le document
    documents = processor.load_file(document_path)
    
    # Ajouter au vector store
    vector_store.add_documents(documents)
    
    # Vider le cache si n√©cessaire
    if rag_tool.cache:
        rag_tool.cache.clear()
    
    logger.info(f"Added {len(documents)} chunks from {document_path}")
```

### 2. Optimisation des chunks

```python
def optimize_chunking(document_path: str, target_chunk_size: int = 400):
    """Optimiser la taille des chunks pour un document sp√©cifique."""
    # Analyser le document
    with open(document_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Tester diff√©rentes tailles de chunks
    chunk_sizes = [200, 300, 400, 500, 600]
    results = {}
    
    for size in chunk_sizes:
        processor = DocumentProcessor(chunk_size=size, chunk_overlap=size//8)
        chunks = processor.load_file(document_path)
        
        # √âvaluer la qualit√© (exemple simplifi√©)
        avg_chunk_length = sum(len(chunk.page_content) for chunk in chunks) / len(chunks)
        results[size] = {
            "num_chunks": len(chunks),
            "avg_length": avg_chunk_length,
            "coverage": len(chunks) * avg_chunk_length / len(content)
        }
    
    return results
```

### 3. Filtrage de contenu

```python
def filter_documents(documents: List[Document], filters: dict) -> List[Document]:
    """Filtrer les documents selon des crit√®res."""
    filtered_docs = []
    
    for doc in documents:
        # V√©rifier les filtres
        include_doc = True
        
        if "min_length" in filters:
            if len(doc.page_content) < filters["min_length"]:
                include_doc = False
        
        if "max_length" in filters:
            if len(doc.page_content) > filters["max_length"]:
                include_doc = False
        
        if "keywords" in filters:
            if not any(keyword in doc.page_content.lower() 
                      for keyword in filters["keywords"]):
                include_doc = False
        
        if include_doc:
            filtered_docs.append(doc)
    
    return filtered_docs
```

## üö® Gestion des erreurs

### 1. Erreurs d'embedding

```python
def handle_embedding_error(error: Exception, query: str):
    """G√©rer les erreurs de g√©n√©ration d'embeddings."""
    logger.error(f"Embedding error for query '{query}': {error}")
    
    # Fallback vers recherche par mots-cl√©s
    try:
        keyword_results = keyword_search(query, k=2)
        logger.info("Using keyword search fallback")
        return keyword_results
    except Exception as fallback_error:
        logger.error(f"Fallback also failed: {fallback_error}")
        return []
```

### 2. Erreurs de recherche

```python
def handle_search_error(error: Exception, query: str):
    """G√©rer les erreurs de recherche vectorielle."""
    logger.error(f"Search error for query '{query}': {error}")
    
    # Retourner une r√©ponse g√©n√©rique
    return [{
        "text": "Je ne peux pas acc√©der √† l'information demand√©e pour le moment. "
                "Veuillez reformuler votre question ou contacter le support Airtel.",
        "source": "fallback",
        "score": 0.0
    }]
```

## üéØ Bonnes pratiques

### 1. Optimisation des performances
- **Cache intelligent** : Utiliser le cache RAG pour les requ√™tes r√©p√©t√©es
- **Chunks optimis√©s** : Taille de chunks adapt√©e au contenu
- **Pr√©chargement** : Charger les documents au d√©marrage
- **Recherche hybride** : Combiner similarit√© vectorielle et mots-cl√©s

### 2. Qualit√© des r√©ponses
- **Contexte appropri√©** : S√©lectionner le bon nombre de chunks
- **Filtrage** : √âliminer les r√©sultats non pertinents
- **Validation** : V√©rifier la coh√©rence des r√©ponses
- **Feedback** : Collecter les retours utilisateurs

### 3. Maintenance
- **Mise √† jour des documents** : Maintenir la base de connaissances √† jour
- **Monitoring** : Surveiller les m√©triques de performance
- **Optimisation continue** : Ajuster les param√®tres selon l'usage
- **Backup** : Sauvegarder r√©guli√®rement les index

## üîó Ressources utiles

- [Syst√®me de pr√©chargement](preloading.md)
- [Architecture m√©moire](memory.md)
- [Configuration des performances](../configuration/performance.md)
- [Guide de debugging](../troubleshooting/debugging.md)
- [Probl√®mes courants](../troubleshooting/common-issues.md)
